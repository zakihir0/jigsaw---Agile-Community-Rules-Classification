{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":12726948,"sourceType":"datasetVersion","datasetId":8044304},{"sourceId":12762469,"sourceType":"datasetVersion","datasetId":8067935},{"sourceId":252850661,"sourceType":"kernelVersion"},{"sourceId":252853424,"sourceType":"kernelVersion"},{"sourceId":171496,"sourceType":"modelInstanceVersion","modelInstanceId":145960,"modelId":164048},{"sourceId":171638,"sourceType":"modelInstanceVersion","modelInstanceId":146086,"modelId":164048},{"sourceId":426330,"sourceType":"modelInstanceVersion","modelInstanceId":347541,"modelId":368803},{"sourceId":523492,"sourceType":"modelInstanceVersion","modelInstanceId":411182,"modelId":429004}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n### References\n\n*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b","metadata":{}},{"cell_type":"markdown","source":"### I want to say thanks to @neibyr for your interesting idea: [Retrieve by Qwen3Embedding](http://https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b) ","metadata":{}},{"cell_type":"markdown","source":"This version changes the lr for training Qwen 3 0.5b. ","metadata":{}},{"cell_type":"markdown","source":"# 1. Test time train Qwen 2.5 0.5b","metadata":{}},{"cell_type":"code","source":"%%writefile constants.py\nBASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\nLORA_PATH = \"output/\"\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\nPOSITIVE_ANSWER = \"Yes\"\nNEGATIVE_ANSWER = \"No\"\nCOMPLETE_PHRASE = \"Answer:\"\nBASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:19:27.471016Z","iopub.execute_input":"2025-08-19T08:19:27.471257Z","iopub.status.idle":"2025-08-19T08:19:27.480531Z","shell.execute_reply.started":"2025-08-19T08:19:27.471232Z","shell.execute_reply":"2025-08-19T08:19:27.479686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nfrom datasets import Dataset\nfrom constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\nimport random, numpy as np\nrandom.seed(42)\nnp.random.seed(42)\n\n\ndef build_prompt(row):\n    return f\"\"\"\n{BASE_PROMPT}\n\nSubreddit: r/{row[\"subreddit\"]}\nRule: {row[\"rule\"]}\nExamples:\n1) {row[\"positive_example\"]}\n{COMPLETE_PHRASE} Yes\n\n2) {row[\"negative_example\"]}\n{COMPLETE_PHRASE} No\n\n---\nComment: {row[\"body\"]}\n{COMPLETE_PHRASE}\"\"\"\n\n\ndef get_dataframe_to_train(data_path):\n    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n\n    flatten = []\n\n    # ---------- 处理训练集 ----------\n    train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n                              \"positive_example_1\",\"positive_example_2\",\n                              \"negative_example_1\",\"negative_example_2\"]].copy()\n\n    # 随机选 positive_example 和 negative_example\n    train_df[\"positive_example\"] = np.where(\n        np.random.rand(len(train_df)) < 0.5,\n        train_df[\"positive_example_1\"],\n        train_df[\"positive_example_2\"]\n    )\n    train_df[\"negative_example\"] = np.where(\n        np.random.rand(len(train_df)) < 0.5,\n        train_df[\"negative_example_1\"],\n        train_df[\"negative_example_2\"]\n    )\n\n    # 删除原来的候选列\n    train_df.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n                           \"negative_example_1\",\"negative_example_2\"], inplace=True)\n\n    flatten.append(train_df)\n\n    # ---------- 处理测试集 ----------\n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n                                        \"positive_example_1\",\"positive_example_2\",\n                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n\n            if violation_type == \"positive\":\n                # body 用当前 positive_example\n                body_col = f\"positive_example_{i}\"\n                other_positive_col = f\"positive_example_{3-i}\"  # 另一个 positive\n                sub_dataset[\"body\"] = sub_dataset[body_col]\n                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n                # negative_example 随机选\n                sub_dataset[\"negative_example\"] = np.where(\n                    np.random.rand(len(sub_dataset)) < 0.5,\n                    sub_dataset[\"negative_example_1\"],\n                    sub_dataset[\"negative_example_2\"]\n                )\n                sub_dataset[\"rule_violation\"] = 1\n\n            else:  # violation_type == \"negative\"\n                body_col = f\"negative_example_{i}\"\n                other_negative_col = f\"negative_example_{3-i}\"\n                sub_dataset[\"body\"] = sub_dataset[body_col]\n                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n                sub_dataset[\"positive_example\"] = np.where(\n                    np.random.rand(len(sub_dataset)) < 0.5,\n                    sub_dataset[\"positive_example_1\"],\n                    sub_dataset[\"positive_example_2\"]\n                )\n                sub_dataset[\"rule_violation\"] = 0\n\n            # 删除原来的候选列\n            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n\n            flatten.append(sub_dataset)\n\n    # 合并所有 DataFrame\n    dataframe = pd.concat(flatten, axis=0)\n    dataframe = dataframe.drop_duplicates(ignore_index=True)\n\n    return dataframe\n\n\n\ndef build_dataset(dataframe):\n    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n\n    columns = [\"prompt\"]\n    if \"rule_violation\" in dataframe:\n        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n            {\n                1: POSITIVE_ANSWER,\n                0: NEGATIVE_ANSWER,\n            }\n        )\n        columns.append(\"completion\")\n\n    dataframe = dataframe[columns]\n    dataset = Dataset.from_pandas(dataframe)\n    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:19:27.482554Z","iopub.execute_input":"2025-08-19T08:19:27.48278Z","iopub.status.idle":"2025-08-19T08:19:27.497054Z","shell.execute_reply.started":"2025-08-19T08:19:27.482758Z","shell.execute_reply":"2025-08-19T08:19:27.496314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\nimport pandas as pd\n\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom tqdm.auto import tqdm\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom utils import build_dataset, get_dataframe_to_train\nfrom constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n\n\ndef main():\n    dataframe = get_dataframe_to_train(DATA_PATH)\n    train_dataset = build_dataset(dataframe)\n    \n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.1,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n    )\n    \n    training_args = SFTConfig(\n        num_train_epochs=1,\n        \n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        \n        optim=\"paged_adamw_8bit\",\n        learning_rate=1e-4, #keep high, lora usually likes high. \n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        \n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.03,\n        \n        bf16=is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        \n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n        save_strategy=\"no\",\n        report_to=\"none\",\n    \n        completion_only_loss=True,\n        packing=False,\n        remove_unused_columns=False,\n    )\n    \n    trainer = SFTTrainer(\n        BASE_MODEL_PATH,\n        args=training_args,\n        train_dataset=train_dataset,\n        peft_config=lora_config,\n    )\n    \n    trainer.train()\n    trainer.save_model(LORA_PATH)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:19:27.49774Z","iopub.execute_input":"2025-08-19T08:19:27.498244Z","iopub.status.idle":"2025-08-19T08:19:27.51202Z","shell.execute_reply.started":"2025-08-19T08:19:27.498174Z","shell.execute_reply":"2025-08-19T08:19:27.511226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile inference.py\nimport os\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nimport vllm\nimport torch\nimport pandas as pd\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom vllm.lora.request import LoRARequest\nfrom utils import build_dataset\nfrom constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\nimport random\nimport multiprocessing as mp\n\n\ndef run_inference_on_device(df_slice):\n    \"\"\"在当前进程可见的 GPU 上跑 vLLM 推理\"\"\"\n    llm = vllm.LLM(\n        BASE_MODEL_PATH,\n        quantization=\"gptq\",\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2836,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=64,\n    )\n\n    tokenizer = llm.get_tokenizer()\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n\n    test_dataset = build_dataset(df_slice)\n    texts = test_dataset[\"prompt\"]\n\n    outputs = llm.generate(\n        texts,\n        vllm.SamplingParams(\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=2,\n        ),\n        use_tqdm=True,\n        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n    )\n\n    log_probs = [\n        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n        for out in outputs\n    ]\n    predictions = pd.DataFrame(log_probs)[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n    return predictions\n\n\ndef worker(device_id, df_slice, return_dict):\n    # 限制该进程只看到一张 GPU\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n\n    preds = run_inference_on_device(df_slice)\n    return_dict[device_id] = preds\n\n\ndef main():\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n\n    # 随机选择例子\n    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]),\n        axis=1\n    )\n    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]),\n        axis=1\n    )\n    test_dataframe = test_dataframe.drop(\n        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n        errors=\"ignore\"\n    )\n\n    # 切分数据\n    mid = len(test_dataframe) // 2\n    df0 = test_dataframe.iloc[:mid].reset_index(drop=True)\n    df1 = test_dataframe.iloc[mid:].reset_index(drop=True)\n\n    manager = mp.Manager()\n    return_dict = manager.dict()\n\n    # 两个进程并行\n    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n    p0.start()\n    p1.start()\n    p0.join()\n    p1.join()\n\n    # 合并结果\n    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n\n    # 构建 submission\n    submission = predictions[[\"row_id\", POSITIVE_ANSWER]].rename(columns={POSITIVE_ANSWER: \"rule_violation\"})\n    rq = submission['rule_violation'].rank(method='average') / (len(submission) + 1)\n    submission['rule_violation'] = rq\n\n    submission.to_csv(\"submission_qwen.csv\", index=False)\n    print(\"✅ Saved submission_qwen.csv\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:19:27.513061Z","iopub.execute_input":"2025-08-19T08:19:27.513252Z","iopub.status.idle":"2025-08-19T08:19:27.530252Z","shell.execute_reply.started":"2025-08-19T08:19:27.513237Z","shell.execute_reply":"2025-08-19T08:19:27.52958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile accelerate_config.yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  gradient_clipping: 1.0\n  train_batch_size: 64\n  train_micro_batch_size_per_gpu: 4\n  \n  zero_stage: 2\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: false\n  \n  stage3_gather_16bit_weights_on_model_save: false\n  stage3_max_live_parameters: 1e8\n  stage3_max_reuse_distance: 1e8\n  stage3_prefetch_bucket_size: 5e7\n  stage3_param_persistence_threshold: 1e5\n  \n  zero_allow_untested_optimizer: true\n  zero_force_ds_cpu_optimizer: false\n  \n  fp16:\n    enabled: true\n    loss_scale: 0\n    initial_scale_power: 16\n    loss_scale_window: 1000\n    hysteresis: 2\n    min_loss_scale: 1\n  \ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_config:\n  dynamo_backend: INDUCTOR\n  dynamo_use_fullgraph: false\n  dynamo_use_dynamic: false\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:19:27.531072Z","iopub.execute_input":"2025-08-19T08:19:27.531265Z","iopub.status.idle":"2025-08-19T08:19:27.547044Z","shell.execute_reply.started":"2025-08-19T08:19:27.53125Z","shell.execute_reply":"2025-08-19T08:19:27.545719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --config_file accelerate_config.yaml train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:19:27.548014Z","iopub.execute_input":"2025-08-19T08:19:27.548284Z","iopub.status.idle":"2025-08-19T08:29:14.383087Z","shell.execute_reply.started":"2025-08-19T08:19:27.548262Z","shell.execute_reply":"2025-08-19T08:29:14.382223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:29:14.384305Z","iopub.execute_input":"2025-08-19T08:29:14.384566Z","iopub.status.idle":"2025-08-19T08:30:32.269369Z","shell.execute_reply.started":"2025-08-19T08:29:14.384541Z","shell.execute_reply":"2025-08-19T08:30:32.268665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!head submission_qwen.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:30:32.271774Z","iopub.execute_input":"2025-08-19T08:30:32.2721Z","iopub.status.idle":"2025-08-19T08:30:32.392328Z","shell.execute_reply.started":"2025-08-19T08:30:32.272077Z","shell.execute_reply":"2025-08-19T08:30:32.39168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Qwen2.5 14B GPTQ Int4 Inference","metadata":{}},{"cell_type":"code","source":"# ! mkdir -p /tmp/src","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:30:32.393174Z","iopub.execute_input":"2025-08-19T08:30:32.393363Z","iopub.status.idle":"2025-08-19T08:30:32.397179Z","shell.execute_reply.started":"2025-08-19T08:30:32.393344Z","shell.execute_reply":"2025-08-19T08:30:32.396457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile infer_qwen.py\n\nimport os\nimport pandas as pd\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nimport torch\nimport vllm\nimport numpy as np\nfrom vllm.lora.request import LoRARequest\nimport argparse\nfrom scipy.special import softmax\ndf = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n\nMODEL_NAME = \"/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1\"\nLORA_PATH = \"/kaggle/input/lora_14b_gptq_1epoch_r32/keras/default/1\"\nif __name__=='__main__':\n    os.environ[\"VLLM_USE_V1\"] = \"0\"\n\n    llm = vllm.LLM(\n        MODEL_NAME,\n        # quantization='awq',\n        quantization='gptq',\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2836,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=32\n    )\n    tokenizer = llm.get_tokenizer()\n    SYS_PROMPT = \"\"\"\nYou are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n\"\"\"\n    \n    prompts = []\n    for i, row in df.iterrows():\n        text = f\"\"\"\n    r/{row.subreddit}\n    Rule: {row.rule}\n    \n    1) {row.positive_example_1}\n    Violation: Yes\n    \n    2) {row.positive_example_2}\n    Violation: Yes\n    \n    3) {row.negative_example_1}\n    Violation: No\n    \n    4) {row.negative_example_2}\n    Violation: No\n    \n    5) {row.body}\n    \"\"\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT},\n            {\"role\": \"user\", \"content\": text}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,\n        ) + \"Answer:\"\n        prompts.append(prompt)\n    \n    df[\"prompt\"] = prompts\n    \n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=['Yes','No'])\n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=2,\n        ),\n        use_tqdm=True,\n        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n    )\n    logprobs = [\n        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n        for out in outputs\n    ]\n    logit_matrix = pd.DataFrame(logprobs)[['Yes','No']]\n    df = pd.concat([df, logit_matrix], axis=1)\n    \n    df[['Yes',\"No\"]] = df[['Yes',\"No\"]].apply(lambda x: softmax(x.values), axis=1, result_type=\"expand\")\n    df[\"pred\"] = df[\"Yes\"]\n    df['rule_violation'] = df[\"pred\"]\n    df[['row_id', 'rule_violation']].to_csv(\"submission_qwen14b.csv\",index=False)\n    pd.read_csv('submission_qwen14b.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:35:15.834516Z","iopub.execute_input":"2025-08-19T08:35:15.83511Z","iopub.status.idle":"2025-08-19T08:35:15.84078Z","shell.execute_reply.started":"2025-08-19T08:35:15.835087Z","shell.execute_reply":"2025-08-19T08:35:15.840205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %cd /tmp\n!python infer_qwen.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:35:21.794857Z","iopub.execute_input":"2025-08-19T08:35:21.795155Z","iopub.status.idle":"2025-08-19T08:37:44.477733Z","shell.execute_reply.started":"2025-08-19T08:35:21.795135Z","shell.execute_reply":"2025-08-19T08:37:44.476967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Qwen3 0.6b Embedding","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:33:34.901275Z","iopub.execute_input":"2025-08-19T08:33:34.901578Z","iopub.status.idle":"2025-08-19T08:33:35.261511Z","shell.execute_reply.started":"2025-08-19T08:33:34.901546Z","shell.execute_reply":"2025-08-19T08:33:35.26095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile constants.py\nEMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nMODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n\n# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\nEMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n\nCLEAN_TEXT = True\nTOP_K = 2000\nBATCH_SIZE = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:33:35.262252Z","iopub.execute_input":"2025-08-19T08:33:35.262539Z","iopub.status.idle":"2025-08-19T08:33:35.2675Z","shell.execute_reply.started":"2025-08-19T08:33:35.262522Z","shell.execute_reply":"2025-08-19T08:33:35.266687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nimport torch.distributed as dist\n\nfrom datasets import Dataset\nfrom cleantext import clean\nfrom tqdm.auto import tqdm\n\nfrom constants import CLEAN_TEXT\n\n\ndef build_prompt(row):\n    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n\n\ndef cleaner(text):\n    return clean(\n        text,\n        fix_unicode=True,\n        to_ascii=True,\n        lower=False,\n        no_line_breaks=False,\n        no_urls=True,\n        no_emails=True,\n        no_phone_numbers=True,\n        no_numbers=False,\n        no_digits=False,\n        no_currency_symbols=False,\n        no_punct=False,\n        replace_with_url=\"<URL>\",\n        replace_with_email=\"<EMAIL>\",\n        replace_with_phone_number=\"<PHONE>\",\n        lang=\"en\",\n    )\n\n\n\ndef get_dataframe_to_train(data_path):\n    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.6, random_state=42).reset_index(drop=True)\n\n    flatten = []\n    flatten.append(train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]])\n    \n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            sub_dataset = test_dataset[[f\"{violation_type}_example_{i}\", \"rule\", \"subreddit\"]].copy()\n            sub_dataset = sub_dataset.rename(columns={f\"{violation_type}_example_{i}\": \"body\"})\n            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n            flatten.append(sub_dataset)\n\n    dataframe = pd.concat(flatten, axis=0)    \n    dataframe = dataframe.drop_duplicates(ignore_index=True)\n    return dataframe\n\n\ndef prepare_dataframe(dataframe):\n    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n\n    \n    if CLEAN_TEXT:\n        tqdm.pandas(desc=\"cleaner\")\n        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n\n    if \"rule_violation\" in dataframe.columns:\n        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map(\n            {\n                1: 1,\n                0: -1,\n            }\n        )\n\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:33:35.268239Z","iopub.execute_input":"2025-08-19T08:33:35.268996Z","iopub.status.idle":"2025-08-19T08:33:35.281257Z","shell.execute_reply.started":"2025-08-19T08:33:35.268972Z","shell.execute_reply":"2025-08-19T08:33:35.280565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile semantic.py\nimport pandas as pd\nfrom transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import semantic_search, dot_score\nfrom tqdm.auto import tqdm\nfrom peft import PeftModel, PeftConfig\n\n\nfrom utils import get_dataframe_to_train, prepare_dataframe\nfrom constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH\n\n\n\ndef get_scores(test_dataframe):\n    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n    \n    # Load base model\n    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n    \n    # Load adapter configuration and model\n    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n    merged_model = lora_model.merge_and_unload()\n    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n\n    # 4. Tạo lại SentenceTransformer từ encoder đã merge\n    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n\n    print('Done loading model!')\n\n    result = []\n    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n        \n        query_embeddings = embedding_model.encode(\n            sentences=test_dataframe_part[\"prompt\"].tolist(),\n            prompt=EMBEDDING_MODEL_QUERY,\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        document_embeddings = embedding_model.encode(\n            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        test_dataframe_part[\"semantic\"] = semantic_search(\n            query_embeddings,\n            document_embeddings,\n            top_k=TOP_K,\n            score_function=dot_score,\n        )\n        def get_score(semantic):\n            semantic = pd.DataFrame(semantic)\n            semantic = semantic.merge(\n                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n                how=\"left\",\n                left_on=\"corpus_id\",\n                right_on=\"row_id\",\n            )\n            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n            return semantic[\"score\"].sum()\n            \n        tqdm.pandas(desc=f\"Add label for {rule=}\")\n        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n        \n    submission = pd.concat(result, axis=0)\n    return submission\n\n\ndef generate_submission():\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_dataframe = prepare_dataframe(test_dataframe)\n    \n    submission = get_scores(test_dataframe)\n    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n    submission.to_csv(\"submission_qwen3.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    generate_submission()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:33:35.282093Z","iopub.execute_input":"2025-08-19T08:33:35.282338Z","iopub.status.idle":"2025-08-19T08:33:35.297534Z","shell.execute_reply.started":"2025-08-19T08:33:35.282317Z","shell.execute_reply":"2025-08-19T08:33:35.296955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python semantic.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:33:35.298129Z","iopub.execute_input":"2025-08-19T08:33:35.298355Z","iopub.status.idle":"2025-08-19T08:34:47.332614Z","shell.execute_reply.started":"2025-08-19T08:33:35.298336Z","shell.execute_reply":"2025-08-19T08:34:47.331851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. ENSEMBLE RESULT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nq = pd.read_csv('submission_qwen.csv')\nl = pd.read_csv('submission_qwen3.csv')\nm = pd.read_csv('submission_qwen14b.csv')\n\n\nrq = q['rule_violation'].rank(method='average') / (len(q)+1)\nrl = l['rule_violation'].rank(method='average') / (len(l)+1)\nrm = m['rule_violation'].rank(method='average') / (len(m)+1)\n\n\nblend = 0.5*rq + 0.3*rl + 0.2*rm   # or tune the rank-weights with a tiny grid using OOF\nq['rule_violation'] = blend\nq.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:37:55.55606Z","iopub.execute_input":"2025-08-19T08:37:55.556874Z","iopub.status.idle":"2025-08-19T08:37:55.57566Z","shell.execute_reply.started":"2025-08-19T08:37:55.556845Z","shell.execute_reply":"2025-08-19T08:37:55.574913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\npd.read_csv('/kaggle/working/submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T08:38:00.077552Z","iopub.execute_input":"2025-08-19T08:38:00.078342Z","iopub.status.idle":"2025-08-19T08:38:00.098882Z","shell.execute_reply.started":"2025-08-19T08:38:00.078316Z","shell.execute_reply":"2025-08-19T08:38:00.098349Z"}},"outputs":[],"execution_count":null}]}